{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torchvision.transforms import Compose\n",
    "import os\n",
    "import cv2 as cv\n",
    "import PIL\n",
    "from utils.data_utils import load_image\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_ANNOT_DIR = \"dataset/training_data/annotations\"\n",
    "max_len = 0\n",
    "skipped = []\n",
    "for annot in os.listdir(ROOT_ANNOT_DIR):\n",
    "    json_path = os.path.join(ROOT_ANNOT_DIR, annot)\n",
    "    try:\n",
    "        with open(json_path,\"r\") as f:\n",
    "            data = json.load(f)\n",
    "        max_len = max(max_len, len(data[\"form\"]))\n",
    "    except:\n",
    "        skipped.append(json_path)\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(ROOT_ANNOT_DIR))-len(skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FUNSD(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        img_dir_path = os.path.join(root_dir,\"images\")\n",
    "        annotation_dir_path = os.path.join(root_dir, \"annotations\")\n",
    "        self.root_dir = root_dir\n",
    "        self.img_dir = os.listdir(img_dir_path)\n",
    "        self.annot_dir = os.listdir(annotation_dir_path)\n",
    "        self.max_annot_len, self.no_of_data = self._check_max()\n",
    "\n",
    "    def _check_max(self) -> Tuple[int ,int]:\n",
    "        max_len = 0\n",
    "        skipped = []\n",
    "        for annot in os.listdir(self.root_dir):\n",
    "            json_path = os.path.join(self.root_dir, annot)\n",
    "            try:\n",
    "                with open(json_path,\"r\") as f:\n",
    "                    data = json.load(f)\n",
    "                max_len = max(max_len, len(data[\"form\"]))\n",
    "            except:\n",
    "                skipped.append(json_path)\n",
    "                continue\n",
    "        return max_len, len(os.listdir(self.annot_dir)) - len(skipped)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.no_of_data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.root_dir, self.img_dir[idx])\n",
    "        annotation = os.path.join(self.root_dir, self.annot_dir[idx])\n",
    "        tokens = []\n",
    "        bboxes = []\n",
    "        ner_tags = []\n",
    "        with open(annotation, \"r\", encoding=\"utf8\") as f:\n",
    "            data = json.load(f)\n",
    "        # image_path = os.path.join(img_dir, file)\n",
    "        # image_path = image_path.replace(\"json\", \"png\")\n",
    "        image, size = load_image(image_path)\n",
    "        for item in data[\"form\"]:\n",
    "            cur_line_bboxes = []\n",
    "            words, label = item[\"words\"], item[\"label\"]\n",
    "            words = [w for w in words if w[\"text\"].strip() != \"\"]\n",
    "            if len(words) == 0:\n",
    "                continue\n",
    "            if label == \"other\":\n",
    "                for w in words:\n",
    "                    tokens.append(w[\"text\"])\n",
    "                    ner_tags.append(\"O\")\n",
    "                    cur_line_bboxes.append(normalize_bbox(w[\"box\"], size))\n",
    "            else:\n",
    "                tokens.append(words[0][\"text\"])\n",
    "                ner_tags.append(\"B-\" + label.upper())\n",
    "                cur_line_bboxes.append(normalize_bbox(words[0][\"box\"], size))\n",
    "                for w in words[1:]:\n",
    "                    tokens.append(w[\"text\"])\n",
    "                    ner_tags.append(\"I-\" + label.upper())\n",
    "                    cur_line_bboxes.append(normalize_bbox(w[\"box\"], size))\n",
    "            # by default: --segment_level_layout 1\n",
    "            # if do not want to use segment_level_layout, comment the following line\n",
    "            cur_line_bboxes = self.get_line_bbox(cur_line_bboxes)\n",
    "            # box = normalize_bbox(item[\"box\"], size)\n",
    "            # cur_line_bboxes = [box for _ in range(len(words))]\n",
    "            bboxes.extend(cur_line_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         ...,\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255]],\n",
      "\n",
      "        [[255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         ...,\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255]],\n",
      "\n",
      "        [[255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         ...,\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         ...,\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255]],\n",
      "\n",
      "        [[255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         ...,\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255]],\n",
      "\n",
      "        [[255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         ...,\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255],\n",
      "         [255, 255, 255]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "img, dim = load_image(\"dataset/training_data/images/00040534.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 777)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
